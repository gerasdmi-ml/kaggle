{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tabpfn --no-index --find-links=file:///kaggle/input/pip-packages-icr/pip-packages\n!mkdir -p /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff\n!cp /kaggle/input/pip-packages-icr/pip-packages/prior_diff_real_checkpoint_n_0_epoch_100.cpkt /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff/\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder,normalize\nfrom sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.impute import SimpleImputer\nimport imblearn\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nimport xgboost\nimport inspect\nfrom collections import defaultdict\nfrom tabpfn import TabPFNClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-08-06T06:46:13.161457Z","iopub.execute_input":"2023-08-06T06:46:13.161861Z","iopub.status.idle":"2023-08-06T06:46:29.782110Z","shell.execute_reply.started":"2023-08-06T06:46:13.161825Z","shell.execute_reply":"2023-08-06T06:46:29.781032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## 0 Preparation \n\ntrain = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/train.csv')\n#train.drop([93,145,186,193,371,434,500,559,229,412,467], axis=0, inplace=True)  == downgrade to 1.95\ntest = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/test.csv')\nsample = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv')\ngreeks = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/greeks.csv')\n\nlb = LabelEncoder()\ntrain['EJ'] = lb.fit_transform(train['EJ']).astype(float)\ntest['EJ'] = lb.fit_transform(test['EJ']).astype(float)\n\ndel_list = list(train.loc[train['EJ']==0]['Id'])\ngr_del_list = list(greeks[greeks['Id'].isin(del_list)].index)\ntr_del_list = train[train['Id'].isin(del_list)].index\ngreeks.drop(index=gr_del_list, axis=0, inplace=True)\ntrain.drop(tr_del_list, axis=0, inplace=True)\n\npredictor_columns = [n for n in train.columns if n != 'Class' and n != 'Id']\n\nx= train[predictor_columns]\ny = train['Class']\n\nfrom sklearn.model_selection import KFold as KF, GridSearchCV\ncv_outer = KF(n_splits = 10, shuffle=True, random_state=42)\ncv_inner = KF(n_splits = 5, shuffle=True, random_state=42)\n\nfrom sklearn.metrics import log_loss\ndef balanced_log_loss(y_true, y_pred):\n    nc = np.bincount(y_true)\n    return log_loss(y_true, y_pred, sample_weight = 1/nc[y_true], eps=1e-15)\n\n\ndef balanced_log_loss_e(y_true, y_pred):\n    hard_cases =[]\n    hard_scores =[]\n    hard_scores2 =[]\n    nc = np.bincount(y_true)\n    #print (type(y_true), type(y_pred), len(y_true), len (y_pred))\n    y_tt = list (y_true)\n    y_pp = list (y_pred)\n    for i, element in enumerate(y_pp):\n        if abs(y_tt[i] - y_pp[i]) > 0.4:\n            hard_cases.append(i)\n            hard_scores.append (y_tt[i])\n            hard_scores2.append (y_pp[i])\n    return log_loss(y_true, y_pred, sample_weight = 1/nc[y_true], eps=1e-15), hard_cases, hard_scores, hard_scores2\n\nclass Ensemble():\n    def __init__(self):\n        self.imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n        #self.imputer = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n\n        self.classifiers =[xgboost.XGBClassifier(n_estimators=200,max_depth=3,learning_rate=0.2,subsample=0.9,colsample_bytree=0.85),\n                           xgboost.XGBClassifier(),\n                           TabPFNClassifier(N_ensemble_configurations=4),\n                          TabPFNClassifier(N_ensemble_configurations=4)]\n        \n        #self.classifiers =[xgboost.XGBClassifier(n_estimators=200,max_depth=3,learning_rate=0.2,subsample=0.9,colsample_bytree=0.85)]\n    \n    def fit(self,X,y):\n        y = y.values\n        unique_classes, y = np.unique(y, return_inverse=True)\n        self.classes_ = unique_classes\n        X = self.imputer.fit_transform(X)\n        for classifier in self.classifiers:\n            if classifier==self.classifiers[2] or classifier==self.classifiers[3]:\n                classifier.fit(X,y,overwrite_warning =True)\n            else :\n                classifier.fit(X, y)\n     \n    def predict_proba(self, x):\n        x = self.imputer.transform(x)\n        probabilities = np.stack([classifier.predict_proba(x) for classifier in self.classifiers])\n        averaged_probabilities = np.mean(probabilities, axis=0)\n        class_0_est_instances = averaged_probabilities[:, 0].sum()\n        others_est_instances = averaged_probabilities[:, 1:].sum()\n        # Weighted probabilities based on class imbalance\n        new_probabilities = averaged_probabilities * np.array([[1/(class_0_est_instances if i==0 else others_est_instances) for i in range(averaged_probabilities.shape[1])]])\n        return new_probabilities / np.sum(new_probabilities, axis=1, keepdims=1) \n    \n\ndef training(model, x,y,y_meta):\n    outer_results = list()\n    best_loss = np.inf\n    split = 0\n    splits = 5\n    models = []\n    scores =[]\n    for train_idx,val_idx in tqdm(cv_inner.split(x), total = splits):\n        split+=1\n        x_train, x_val = x.iloc[train_idx],x.iloc[val_idx]\n        y_train, y_val = y_meta.iloc[train_idx], y.iloc[val_idx]\n                \n        model.fit(x_train, y_train)\n        y_pred = model.predict_proba(x_val)\n        probabilities = np.concatenate((y_pred[:,:1], np.sum(y_pred[:,1:], 1, keepdims=True)), axis=1)\n        p0 = probabilities[:,:1]\n        y_p = np.empty((y_pred.shape[0],))\n        for i in range(y_pred.shape[0]):\n            y_p[i]= p0[i]\n            if p0[i]>=0.5:\n                y_p[i]= False\n            else :\n                y_p[i]=True\n        y_p = y_p.astype(int)\n        loss = balanced_log_loss(y_val,y_p)\n        models.append(model)\n        scores.append(loss)\n        if loss<best_loss:\n            best_model = model\n            best_loss = loss\n            print('best_model_saved')\n        outer_results.append(loss)\n        print('>val_loss=%.15f, split = %.1f' % (loss,split))\n    print('LOSS: %.5f' % (np.mean(outer_results)))\n    return best_model, models, scores\n\ndef training_nf(model, x,y,y_meta):\n    models = []\n    scores =[]\n    model.fit(x, y)\n    best_model = model\n    for i in range (5):\n        models.append(model)\n        scores.append(i)\n    return best_model, models, scores\n    \n    for train_idx,val_idx in tqdm(cv_inner.split(x), total = splits):\n        split+=1\n        x_train, x_val = x.iloc[train_idx],x.iloc[val_idx]\n        y_train, y_val = y_meta.iloc[train_idx], y.iloc[val_idx]\n                \n        model.fit(x_train, y_train)\n        y_pred = model.predict_proba(x_val)\n        probabilities = np.concatenate((y_pred[:,:1], np.sum(y_pred[:,1:], 1, keepdims=True)), axis=1)\n        p0 = probabilities[:,:1]\n        y_p = np.empty((y_pred.shape[0],))\n        for i in range(y_pred.shape[0]):\n            y_p[i]= p0[i]\n            if p0[i]>=0.5:\n                y_p[i]= False\n            else :\n                y_p[i]=True\n        y_p = y_p.astype(int)\n        loss = balanced_log_loss(y_val,y_p)\n        models.append(model)\n        scores.append(loss)\n        if loss<best_loss:\n            best_model = model\n            best_loss = loss\n            print('best_model_saved')\n        outer_results.append(loss)\n        print('>val_loss=%.15f, split = %.1f' % (loss,split))\n    print('LOSS: %.5f' % (np.mean(outer_results)))\n    return best_model, models, scores\n\n\n## 1 BASIC TRAIN \n\nfrom datetime import datetime\ntimes = greeks.Epsilon.copy()\ntimes[greeks.Epsilon != 'Unknown'] = greeks.Epsilon[greeks.Epsilon != 'Unknown'].map(lambda x: datetime.strptime(x,'%m/%d/%Y').toordinal())\ntimes[greeks.Epsilon == 'Unknown'] = np.nan\n\ntrain_pred_and_time = pd.concat((train, times), axis=1)\n\n'''\ntest['A'] = test['AB']*10000+test['AF']+test['AM']*100+test['AY']*10000\ntest['B'] = test['BC']*1000+test['BQ']*1000+test['BR']+test['BZ']\ntest['D'] = test['DU']*500\ntest['E'] = test['EH']*1000\ntest['F'] = test['FD ']*100 + test['FE']+ test['FL']*1000  + test['FR']*100\npredictor_columns.append('A')     # useful\npredictor_columns.append('B')     # useful\npredictor_columns.append('D')     # useful\npredictor_columns.append('E')     # useful\npredictor_columns.append('F')     # useful\n\ntest['C']= test['CC'] + test['CD ']+ test['CF']+test['CH']+test['CL']+test['CR']+test['CU']+test['CW ']\ntest['F']= test['FC'] + test['FD ']+ test['FE']+test['FI']+test['FL']+test['FR']+test['FS']\n#basic_features.append('id2')   # useful\npredictor_columns.append('C')     # useful\npredictor_columns.append('F')     # useful\n\nif 'DA' in predictor_columns: predictor_columns.remove('DA') # useful\n#if 'BN' in predictor_columns: predictor_columns.remove('BN') # useful\nif 'BP' in predictor_columns: predictor_columns.remove('BP') # useful\nif 'CC' in predictor_columns: predictor_columns.remove('CC') # useful\nif 'CW ' in predictor_columns: predictor_columns.remove('CW ') # useful\nif 'DI' in predictor_columns: predictor_columns.remove('DI') # useful\nif 'GH' in predictor_columns: predictor_columns.remove('GH') # useful\nif 'BC' in predictor_columns: predictor_columns.remove('BC') # useful\nif 'AF' in predictor_columns: predictor_columns.remove('AF') # useful\n'''\ntest_predictors = test[predictor_columns]\n\n\nfirst_category = test_predictors.EJ.unique()[0]\ntest_predictors.EJ = test_predictors.EJ.eq(first_category).astype('int')\ntest_pred_and_time = np.concatenate((test_predictors, np.zeros((len(test_predictors), 1)) + train_pred_and_time.Epsilon.max() + 1), axis=1)\n\nros = RandomOverSampler(random_state=42)\n\ntrain_ros, y_ros = ros.fit_resample(train_pred_and_time, greeks.Alpha)\n#print('Original dataset shape')\n#print(greeks.Alpha.value_counts())\n#print('Resample dataset shape')\n#print( y_ros.value_counts())\n\nx_ros = train_ros.drop(['Class', 'Id'],axis=1)\n\n\n'''\nx_ros['A'] = x_ros['AB']*10000+x_ros['AF']+x_ros['AM']*100+x_ros['AY']*10000\nx_ros['B'] = x_ros['BC']*1000+x_ros['BQ']*1000+x_ros['BR']+x_ros['BZ']\nx_ros['D'] = x_ros['DU']*500\nx_ros['E'] = x_ros['EH']*1000\nx_ros['F'] = x_ros['FD ']*100 + x_ros['FE']+ x_ros['FL']*1000  + x_ros['FR']*100\n\nx_ros['C']= x_ros['CC'] + x_ros['CD ']+ x_ros['CF']+x_ros['CH']+x_ros['CL']+x_ros['CR']+x_ros['CU']+x_ros['CW ']\nx_ros['F']= x_ros['FC'] + x_ros['FD ']+ x_ros['FE']+x_ros['FI']+x_ros['FL']+x_ros['FR']+x_ros['FS']                          # test['FI']\n\nx_ros.drop(['DA','BP','CC','CW ','DI','GH','BC','AF'],axis=1, inplace=True)\n'''\n\ny_ = train_ros.Class\nyt = Ensemble()\nm, all_models, all_scores = training_nf(yt,x_ros,y_,y_ros)\n\n# 2 predict for submission \n\ny_pred = m.predict_proba(test_pred_and_time)\nprobabilities = np.concatenate((y_pred[:,:1], np.sum(y_pred[:,1:], 1, keepdims=True)), axis=1)\n\n\nall_scores_s = all_scores     \nall_scores_s.sort()\nindex_of_second = all_scores.index(all_scores_s[1])\nindex_of_third = all_scores.index(all_scores_s[2])\nindex_of_forth = all_scores.index(all_scores_s[3])\nindex_of_five = all_scores.index(all_scores_s[4])\n\n\ny_pred2 = all_models[index_of_second].predict_proba(test_pred_and_time)\nprobabilities2 = np.concatenate((y_pred2[:,:1], np.sum(y_pred2[:,1:], 1, keepdims=True)), axis=1)\ny_pred3 = all_models[index_of_third].predict_proba(test_pred_and_time)\nprobabilities3 = np.concatenate((y_pred3[:,:1], np.sum(y_pred3[:,1:], 1, keepdims=True)), axis=1)\ny_pred4 = all_models[index_of_forth].predict_proba(test_pred_and_time)\nprobabilities4 = np.concatenate((y_pred4[:,:1], np.sum(y_pred4[:,1:], 1, keepdims=True)), axis=1)\ny_pred5 = all_models[index_of_five].predict_proba(test_pred_and_time)\nprobabilities5 = np.concatenate((y_pred5[:,:1], np.sum(y_pred5[:,1:], 1, keepdims=True)), axis=1)\n\np0 = (probabilities[:,:1] + probabilities2[:,:1]+ probabilities3[:,:1]+ probabilities4[:,:1]+ probabilities5[:,:1])/5\n\n###p0[p0 > 0.10] = 1 # пробуем\n###p0[p0 < 0.01] = 0\n\n'''\nzero_list = list(test.loc[train_pred_and_time['BQ'].isnull()].index)\nfor z in zero_list : p0[z] = 1\nzero_list = list(test.loc[train_pred_and_time['BN'] < 15.5364].index)\nfor z in zero_list : p0[z] = 1\n'''\n\nsubmission_0 = pd.DataFrame(test[\"Id\"], columns=[\"Id\"])\nsubmission_0[\"class_0\"] = p0\nsubmission_0[\"class_1\"] = 1 - p0\nsubmission_0.to_csv('submission_0.csv', index=False)\n\nprint ('------------------- submisson 0 completed ')\n\n\n## 3 predict  for all TRAIN data \n\npredictor_columns_2 = [n for n in train_pred_and_time.columns if n != 'Class' and n != 'Id']\n\n'''\ntrain_pred_and_time['A'] = train_pred_and_time['AB']*10000+train_pred_and_time['AF']+train_pred_and_time['AM']*100+train_pred_and_time['AY']*10000\ntrain_pred_and_time['B'] = train_pred_and_time['BC']*1000+train_pred_and_time['BQ']*1000+train_pred_and_time['BR']+train_pred_and_time['BZ']\ntrain_pred_and_time['D'] = train_pred_and_time['DU']*500\ntrain_pred_and_time['E'] = train_pred_and_time['EH']*1000\ntrain_pred_and_time['F'] = train_pred_and_time['FD ']*100 + train_pred_and_time['FE']+ train_pred_and_time['FL']*1000  + train_pred_and_time['FR']*100\npredictor_columns_2.append('A')     # useful\npredictor_columns_2.append('B')     # useful\npredictor_columns_2.append('D')     # useful\npredictor_columns_2.append('E')     # useful\npredictor_columns_2.append('F')     # useful\n\ntrain_pred_and_time['C']= train_pred_and_time['CC'] + train_pred_and_time['CD ']+ train_pred_and_time['CF']+train_pred_and_time['CH']+train_pred_and_time['CL']+train_pred_and_time['CR']+train_pred_and_time['CU']+train_pred_and_time['CW ']\ntrain_pred_and_time['F']= train_pred_and_time['FC'] + train_pred_and_time['FD ']+ train_pred_and_time['FE']+train_pred_and_time['FI']+train_pred_and_time['FL']+train_pred_and_time['FR']+train_pred_and_time['FS']\n#basic_features.append('id2')   # useful\npredictor_columns_2.append('C')     # useful\npredictor_columns_2.append('F')     # useful\n\nif 'DA' in predictor_columns_2: predictor_columns_2.remove('DA') # useful\n#if 'BN' in predictor_columns_2: predictor_columns_2.remove('BN') # useful\nif 'BP' in predictor_columns_2: predictor_columns_2.remove('BP') # useful\nif 'CC' in predictor_columns_2: predictor_columns_2.remove('CC') # useful\nif 'CW ' in predictor_columns_2: predictor_columns_2.remove('CW ') # useful\nif 'DI' in predictor_columns_2: predictor_columns_2.remove('DI') # useful\nif 'GH' in predictor_columns_2: predictor_columns_2.remove('GH') # useful\nif 'BC' in predictor_columns_2: predictor_columns_2.remove('BC') # useful\nif 'AF' in predictor_columns_2: predictor_columns_2.remove('AF') # useful\n'''\n\ny_pred=  m.predict_proba(train_pred_and_time[predictor_columns_2])\nprobabilities = np.concatenate((y_pred[:,:1], np.sum(y_pred[:,1:], 1, keepdims=True)), axis=1)\n\ny_pred2 = all_models[index_of_second].predict_proba(train_pred_and_time[predictor_columns_2])\nprobabilities2 = np.concatenate((y_pred2[:,:1], np.sum(y_pred2[:,1:], 1, keepdims=True)), axis=1)\ny_pred3 = all_models[index_of_third].predict_proba(train_pred_and_time[predictor_columns_2])\nprobabilities3 = np.concatenate((y_pred3[:,:1], np.sum(y_pred3[:,1:], 1, keepdims=True)), axis=1)\ny_pred4 = all_models[index_of_forth].predict_proba(train_pred_and_time[predictor_columns_2])\nprobabilities4 = np.concatenate((y_pred4[:,:1], np.sum(y_pred4[:,1:], 1, keepdims=True)), axis=1)\ny_pred5 = all_models[index_of_five].predict_proba(train_pred_and_time[predictor_columns_2])\nprobabilities5 = np.concatenate((y_pred5[:,:1], np.sum(y_pred5[:,1:], 1, keepdims=True)), axis=1)\n\np0 = (probabilities[:,:1] + probabilities2[:,:1]+ probabilities3[:,:1]+ probabilities4[:,:1]+ probabilities5[:,:1])/5\n\n\n\n\n###p0[p0 > 0.10] = 1 \n###p0[p0 < 0.01] = 0\n\n'''\nzero_list = list(train_pred_and_time.loc[train_pred_and_time['BQ'].isnull()].index)\nfor z in zero_list : p0[z] = 1\nzero_list = list(train_pred_and_time.loc[train_pred_and_time['BN'] < 15.5364].index)\nfor z in zero_list : p0[z] = 1\n'''\n\np1 = 1 - p0\n\n\nprint('overall',balanced_log_loss_e(train.Class,p1.flatten()))\n\n#overall CV 0.0928  --  -- 24 & 64  >30 mins       0.13 LB \n\n# overall (0.02794048828990353,  [103, 134, 205, 229, 295, 299], [0, 0, 0, 0, 0, 0], [0.78, 0.40, 0.61, 0.76, 0.845, 0.5419956709666807])\n# overall (0.020332229109128463, [8, 64, 87, 128, 189], [0, 0, 0, 0, 0], [0.4743714693172232, 0.5984776315191408, 0.8488388713780383, 0.4809832856073687, 0.7367310655403029])\n#overall (9.992007221626415e-16, [], [], []) including thresholds\n#overall (0.006685700891628771, [], [], [])  no thresholds at all ","metadata":{"execution":{"iopub.status.busy":"2023-08-06T06:46:29.784864Z","iopub.execute_input":"2023-08-06T06:46:29.785980Z","iopub.status.idle":"2023-08-06T06:48:10.801136Z","shell.execute_reply.started":"2023-08-06T06:46:29.785944Z","shell.execute_reply":"2023-08-06T06:48:10.800156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## 0 Preparation \n\ntrain = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/train.csv')\n#train.drop([93,145,186,193,371,434,500,559,229,412,467], axis=0, inplace=True)  == downgrade to 1.95\ntest = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/test.csv')\nsample = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv')\ngreeks = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/greeks.csv')\n\nlb = LabelEncoder()\ntrain['EJ'] = lb.fit_transform(train['EJ']).astype(float)\ntest['EJ'] = lb.fit_transform(test['EJ']).astype(float)\n\ndel_list = list(train.loc[train['EJ']==1]['Id'])\ngr_del_list = list(greeks[greeks['Id'].isin(del_list)].index)\ntr_del_list = train[train['Id'].isin(del_list)].index\ngreeks.drop(index=gr_del_list, axis=0, inplace=True)\ntrain.drop(tr_del_list, axis=0, inplace=True)\n\npredictor_columns = [n for n in train.columns if n != 'Class' and n != 'Id']\n\nx= train[predictor_columns]\ny = train['Class']\n\nfrom sklearn.model_selection import KFold as KF, GridSearchCV\ncv_outer = KF(n_splits = 10, shuffle=True, random_state=42)\ncv_inner = KF(n_splits = 5, shuffle=True, random_state=42)\n\nfrom sklearn.metrics import log_loss\ndef balanced_log_loss(y_true, y_pred):\n    nc = np.bincount(y_true)\n    return log_loss(y_true, y_pred, sample_weight = 1/nc[y_true], eps=1e-15)\n\n\ndef balanced_log_loss_e(y_true, y_pred):\n    hard_cases =[]\n    hard_scores =[]\n    hard_scores2 =[]\n    nc = np.bincount(y_true)\n    #print (type(y_true), type(y_pred), len(y_true), len (y_pred))\n    y_tt = list (y_true)\n    y_pp = list (y_pred)\n    for i, element in enumerate(y_pp):\n        if abs(y_tt[i] - y_pp[i]) > 0.4:\n            hard_cases.append(i)\n            hard_scores.append (y_tt[i])\n            hard_scores2.append (y_pp[i])\n    return log_loss(y_true, y_pred, sample_weight = 1/nc[y_true], eps=1e-15), hard_cases, hard_scores, hard_scores2\n\nclass Ensemble():\n    def __init__(self):\n        self.imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n        #self.imputer = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n\n        self.classifiers =[xgboost.XGBClassifier(n_estimators=200,max_depth=3,learning_rate=0.2,subsample=0.9,colsample_bytree=0.85),\n                           xgboost.XGBClassifier(),\n                           TabPFNClassifier(N_ensemble_configurations=4),\n                          TabPFNClassifier(N_ensemble_configurations=4)]\n        \n        #self.classifiers =[xgboost.XGBClassifier(n_estimators=200,max_depth=3,learning_rate=0.2,subsample=0.9,colsample_bytree=0.85)]\n    \n    def fit(self,X,y):\n        y = y.values\n        unique_classes, y = np.unique(y, return_inverse=True)\n        self.classes_ = unique_classes\n        X = self.imputer.fit_transform(X)\n        for classifier in self.classifiers:\n            if classifier==self.classifiers[2] or classifier==self.classifiers[3]:\n                classifier.fit(X,y,overwrite_warning =True)\n            else :\n                classifier.fit(X, y)\n     \n    def predict_proba(self, x):\n        x = self.imputer.transform(x)\n        probabilities = np.stack([classifier.predict_proba(x) for classifier in self.classifiers])\n        averaged_probabilities = np.mean(probabilities, axis=0)\n        class_0_est_instances = averaged_probabilities[:, 0].sum()\n        others_est_instances = averaged_probabilities[:, 1:].sum()\n        # Weighted probabilities based on class imbalance\n        new_probabilities = averaged_probabilities * np.array([[1/(class_0_est_instances if i==0 else others_est_instances) for i in range(averaged_probabilities.shape[1])]])\n        return new_probabilities / np.sum(new_probabilities, axis=1, keepdims=1) \n    \n\ndef training(model, x,y,y_meta):\n    outer_results = list()\n    best_loss = np.inf\n    split = 0\n    splits = 5\n    models = []\n    scores =[]\n    for train_idx,val_idx in tqdm(cv_inner.split(x), total = splits):\n        split+=1\n        x_train, x_val = x.iloc[train_idx],x.iloc[val_idx]\n        y_train, y_val = y_meta.iloc[train_idx], y.iloc[val_idx]\n                \n        model.fit(x_train, y_train)\n        y_pred = model.predict_proba(x_val)\n        probabilities = np.concatenate((y_pred[:,:1], np.sum(y_pred[:,1:], 1, keepdims=True)), axis=1)\n        p0 = probabilities[:,:1]\n        y_p = np.empty((y_pred.shape[0],))\n        for i in range(y_pred.shape[0]):\n            y_p[i]= p0[i]\n            if p0[i]>=0.5:\n                y_p[i]= False\n            else :\n                y_p[i]=True\n        y_p = y_p.astype(int)\n        loss = balanced_log_loss(y_val,y_p)\n        models.append(model)\n        scores.append(loss)\n        if loss<best_loss:\n            best_model = model\n            best_loss = loss\n            print('best_model_saved')\n        outer_results.append(loss)\n        print('>val_loss=%.15f, split = %.1f' % (loss,split))\n    print('LOSS: %.5f' % (np.mean(outer_results)))\n    return best_model, models, scores\n\n## 1 BASIC TRAIN \n\nfrom datetime import datetime\ntimes = greeks.Epsilon.copy()\ntimes[greeks.Epsilon != 'Unknown'] = greeks.Epsilon[greeks.Epsilon != 'Unknown'].map(lambda x: datetime.strptime(x,'%m/%d/%Y').toordinal())\ntimes[greeks.Epsilon == 'Unknown'] = np.nan\n\ntrain_pred_and_time = pd.concat((train, times), axis=1)\n\n'''\ntest['A'] = test['AB']*10000+test['AF']+test['AM']*100+test['AY']*10000\ntest['B'] = test['BC']*1000+test['BQ']*1000+test['BR']+test['BZ']\ntest['D'] = test['DU']*500\ntest['E'] = test['EH']*1000\ntest['F'] = test['FD ']*100 + test['FE']+ test['FL']*1000  + test['FR']*100\npredictor_columns.append('A')     # useful\npredictor_columns.append('B')     # useful\npredictor_columns.append('D')     # useful\npredictor_columns.append('E')     # useful\npredictor_columns.append('F')     # useful\n\ntest['C']= test['CC'] + test['CD ']+ test['CF']+test['CH']+test['CL']+test['CR']+test['CU']+test['CW ']\ntest['F']= test['FC'] + test['FD ']+ test['FE']+test['FI']+test['FL']+test['FR']+test['FS']\n#basic_features.append('id2')   # useful\npredictor_columns.append('C')     # useful\npredictor_columns.append('F')     # useful\n\nif 'DA' in predictor_columns: predictor_columns.remove('DA') # useful\n#if 'BN' in predictor_columns: predictor_columns.remove('BN') # useful\nif 'BP' in predictor_columns: predictor_columns.remove('BP') # useful\nif 'CC' in predictor_columns: predictor_columns.remove('CC') # useful\nif 'CW ' in predictor_columns: predictor_columns.remove('CW ') # useful\nif 'DI' in predictor_columns: predictor_columns.remove('DI') # useful\nif 'GH' in predictor_columns: predictor_columns.remove('GH') # useful\nif 'BC' in predictor_columns: predictor_columns.remove('BC') # useful\nif 'AF' in predictor_columns: predictor_columns.remove('AF') # useful\n'''\ntest_predictors = test[predictor_columns]\n\n\nfirst_category = test_predictors.EJ.unique()[0]\ntest_predictors.EJ = test_predictors.EJ.eq(first_category).astype('int')\ntest_pred_and_time = np.concatenate((test_predictors, np.zeros((len(test_predictors), 1)) + train_pred_and_time.Epsilon.max() + 1), axis=1)\n\nros = RandomOverSampler(random_state=42)\n\ntrain_ros, y_ros = ros.fit_resample(train_pred_and_time, greeks.Alpha)\n#print('Original dataset shape')\n#print(greeks.Alpha.value_counts())\n#print('Resample dataset shape')\n#print( y_ros.value_counts())\n\nx_ros = train_ros.drop(['Class', 'Id'],axis=1)\n\n\n'''\nx_ros['A'] = x_ros['AB']*10000+x_ros['AF']+x_ros['AM']*100+x_ros['AY']*10000\nx_ros['B'] = x_ros['BC']*1000+x_ros['BQ']*1000+x_ros['BR']+x_ros['BZ']\nx_ros['D'] = x_ros['DU']*500\nx_ros['E'] = x_ros['EH']*1000\nx_ros['F'] = x_ros['FD ']*100 + x_ros['FE']+ x_ros['FL']*1000  + x_ros['FR']*100\n\nx_ros['C']= x_ros['CC'] + x_ros['CD ']+ x_ros['CF']+x_ros['CH']+x_ros['CL']+x_ros['CR']+x_ros['CU']+x_ros['CW ']\nx_ros['F']= x_ros['FC'] + x_ros['FD ']+ x_ros['FE']+x_ros['FI']+x_ros['FL']+x_ros['FR']+x_ros['FS']                          # test['FI']\n\nx_ros.drop(['DA','BP','CC','CW ','DI','GH','BC','AF'],axis=1, inplace=True)\n'''\n\ny_ = train_ros.Class\nyt = Ensemble()\nm, all_models, all_scores = training_nf(yt,x_ros,y_,y_ros)\n\n# 2 predict for submission \n\ny_pred = m.predict_proba(test_pred_and_time)\nprobabilities = np.concatenate((y_pred[:,:1], np.sum(y_pred[:,1:], 1, keepdims=True)), axis=1)\n\n\nall_scores_s = all_scores     \nall_scores_s.sort()\nindex_of_second = all_scores.index(all_scores_s[1])\nindex_of_third = all_scores.index(all_scores_s[2])\nindex_of_forth = all_scores.index(all_scores_s[3])\nindex_of_five = all_scores.index(all_scores_s[4])\n\ny_pred2 = all_models[index_of_second].predict_proba(test_pred_and_time)\nprobabilities2 = np.concatenate((y_pred2[:,:1], np.sum(y_pred2[:,1:], 1, keepdims=True)), axis=1)\ny_pred3 = all_models[index_of_third].predict_proba(test_pred_and_time)\nprobabilities3 = np.concatenate((y_pred3[:,:1], np.sum(y_pred3[:,1:], 1, keepdims=True)), axis=1)\ny_pred4 = all_models[index_of_forth].predict_proba(test_pred_and_time)\nprobabilities4 = np.concatenate((y_pred4[:,:1], np.sum(y_pred4[:,1:], 1, keepdims=True)), axis=1)\ny_pred5 = all_models[index_of_five].predict_proba(test_pred_and_time)\nprobabilities5 = np.concatenate((y_pred5[:,:1], np.sum(y_pred5[:,1:], 1, keepdims=True)), axis=1)\n\np0 = (probabilities[:,:1] + probabilities2[:,:1]+ probabilities3[:,:1]+ probabilities4[:,:1]+ probabilities5[:,:1])/5\n\n##p0[p0 > 0.10] = 1 # пробуем\n##p0[p0 < 0.01] = 0\n\n'''\nzero_list = list(test.loc[train_pred_and_time['BQ'].isnull()].index)\nfor z in zero_list : p0[z] = 1\nzero_list = list(test.loc[train_pred_and_time['BN'] < 15.5364].index)\nfor z in zero_list : p0[z] = 1\n'''\n\nsubmission_1 = pd.DataFrame(test[\"Id\"], columns=[\"Id\"])\nsubmission_1[\"class_0\"] = p0\nsubmission_1[\"class_1\"] = 1 - p0\nsubmission_1.to_csv('submission_1.csv', index=False)\n\nprint ('------------------- submisson 1 completed ')\n\nsubmission_0.append(submission_1)\nsubmission_0.to_csv('submission.csv', index=False)\n\nprint ('------------------- GENERAL submisson  completed ')\n\n## 3 predict  for all TRAIN data \n\npredictor_columns_2 = [n for n in train_pred_and_time.columns if n != 'Class' and n != 'Id']\n\n'''\ntrain_pred_and_time['A'] = train_pred_and_time['AB']*10000+train_pred_and_time['AF']+train_pred_and_time['AM']*100+train_pred_and_time['AY']*10000\ntrain_pred_and_time['B'] = train_pred_and_time['BC']*1000+train_pred_and_time['BQ']*1000+train_pred_and_time['BR']+train_pred_and_time['BZ']\ntrain_pred_and_time['D'] = train_pred_and_time['DU']*500\ntrain_pred_and_time['E'] = train_pred_and_time['EH']*1000\ntrain_pred_and_time['F'] = train_pred_and_time['FD ']*100 + train_pred_and_time['FE']+ train_pred_and_time['FL']*1000  + train_pred_and_time['FR']*100\npredictor_columns_2.append('A')     # useful\npredictor_columns_2.append('B')     # useful\npredictor_columns_2.append('D')     # useful\npredictor_columns_2.append('E')     # useful\npredictor_columns_2.append('F')     # useful\n\ntrain_pred_and_time['C']= train_pred_and_time['CC'] + train_pred_and_time['CD ']+ train_pred_and_time['CF']+train_pred_and_time['CH']+train_pred_and_time['CL']+train_pred_and_time['CR']+train_pred_and_time['CU']+train_pred_and_time['CW ']\ntrain_pred_and_time['F']= train_pred_and_time['FC'] + train_pred_and_time['FD ']+ train_pred_and_time['FE']+train_pred_and_time['FI']+train_pred_and_time['FL']+train_pred_and_time['FR']+train_pred_and_time['FS']\n#basic_features.append('id2')   # useful\npredictor_columns_2.append('C')     # useful\npredictor_columns_2.append('F')     # useful\n\nif 'DA' in predictor_columns_2: predictor_columns_2.remove('DA') # useful\n#if 'BN' in predictor_columns_2: predictor_columns_2.remove('BN') # useful\nif 'BP' in predictor_columns_2: predictor_columns_2.remove('BP') # useful\nif 'CC' in predictor_columns_2: predictor_columns_2.remove('CC') # useful\nif 'CW ' in predictor_columns_2: predictor_columns_2.remove('CW ') # useful\nif 'DI' in predictor_columns_2: predictor_columns_2.remove('DI') # useful\nif 'GH' in predictor_columns_2: predictor_columns_2.remove('GH') # useful\nif 'BC' in predictor_columns_2: predictor_columns_2.remove('BC') # useful\nif 'AF' in predictor_columns_2: predictor_columns_2.remove('AF') # useful\n'''\n\ny_pred=  m.predict_proba(train_pred_and_time[predictor_columns_2])\nprobabilities = np.concatenate((y_pred[:,:1], np.sum(y_pred[:,1:], 1, keepdims=True)), axis=1)\n\ny_pred2 = all_models[index_of_second].predict_proba(train_pred_and_time[predictor_columns_2])\nprobabilities2 = np.concatenate((y_pred2[:,:1], np.sum(y_pred2[:,1:], 1, keepdims=True)), axis=1)\ny_pred3 = all_models[index_of_third].predict_proba(train_pred_and_time[predictor_columns_2])\nprobabilities3 = np.concatenate((y_pred3[:,:1], np.sum(y_pred3[:,1:], 1, keepdims=True)), axis=1)\ny_pred4 = all_models[index_of_forth].predict_proba(train_pred_and_time[predictor_columns_2])\nprobabilities4 = np.concatenate((y_pred4[:,:1], np.sum(y_pred4[:,1:], 1, keepdims=True)), axis=1)\ny_pred5 = all_models[index_of_five].predict_proba(train_pred_and_time[predictor_columns_2])\nprobabilities5 = np.concatenate((y_pred5[:,:1], np.sum(y_pred5[:,1:], 1, keepdims=True)), axis=1)\n\np0 = (probabilities[:,:1] + probabilities2[:,:1]+ probabilities3[:,:1]+ probabilities4[:,:1]+ probabilities5[:,:1])/5\n\n\n###p0[p0 > 0.10] = 1 \n###p0[p0 < 0.01] = 0\n\n'''\nzero_list = list(train_pred_and_time.loc[train_pred_and_time['BQ'].isnull()].index)\nfor z in zero_list : p0[z] = 1\nzero_list = list(train_pred_and_time.loc[train_pred_and_time['BN'] < 15.5364].index)\nfor z in zero_list : p0[z] = 1\n'''\n\np1 = 1 - p0\n\n\nprint('overall',balanced_log_loss_e(train.Class,p1.flatten()))\n\n#overall CV 0.0928  --  -- 24 & 64  >30 mins       0.13 LB \n\n# overall (0.02794048828990353,  [103, 134, 205, 229, 295, 299], [0, 0, 0, 0, 0, 0], [0.78, 0.40, 0.61, 0.76, 0.845, 0.5419956709666807])\n# overall (0.020332229109128463, [8, 64, 87, 128, 189], [0, 0, 0, 0, 0], [0.4743714693172232, 0.5984776315191408, 0.8488388713780383, 0.4809832856073687, 0.7367310655403029])\n# overall (0.011763950426382223, [], [], []) no thresholds at all no trhresholds at all, simple model","metadata":{"execution":{"iopub.status.busy":"2023-08-06T06:48:10.802303Z","iopub.execute_input":"2023-08-06T06:48:10.803487Z","iopub.status.idle":"2023-08-06T06:48:46.255824Z","shell.execute_reply.started":"2023-08-06T06:48:10.803432Z","shell.execute_reply":"2023-08-06T06:48:46.254926Z"},"trusted":true},"execution_count":null,"outputs":[]}]}