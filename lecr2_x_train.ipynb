{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10bae8e5",
   "metadata": {},
   "source": [
    "# 5 train xencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20c15c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n"
     ]
    }
   ],
   "source": [
    "# чистая модель, без прогрева, скорость обучения не задана, только 0 и 1, default_activation = sigmoid\n",
    "# эпохи - от 5 раз, л, пока взять небольшое knn = 20-30\n",
    "# кроссэнкодер путается, когда слишком много примеров и опирается на предыдущее обучение, нужно, чтобы он его забыл\n",
    "# добавим descritption - иначе кросс энкодер путается\n",
    "\n",
    "# возьмем новый биэнкодер и новые примеры \n",
    "# merged text - точно как в биэнкодере с пробелами\n",
    "# давай все же оттренируем 5 эпох грязной модели ? и knn нам нужен выше 20\n",
    "# сделаем полноценное k-fold кросс валидацию\n",
    "# возможно одно из условий успеха - постоянная перегенерация примеров на дообученных моделях и снова перегенерация примеров\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from sentence_transformers import SentenceTransformer,InputExample,losses\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers import util\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "print ('started')\n",
    "#x_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2',num_labels=1, max_length=512, default_activation_function=nn.Sigmoid())\n",
    "x_encoder = CrossEncoder('x-final-0311b',num_labels=1, max_length=128, default_activation_function=nn.Sigmoid())\n",
    "\n",
    "BASE_DIR = 'C:/learning'              # временно, для отладки\n",
    "num_epochs = 1\n",
    "#warmup_steps = int(len(train_dataloader) * num_epochs * 0.1) #10% of train data\n",
    "warmup_steps = 0 \n",
    "#lr = 2e-5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c86b2e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 206777/206777 [00:09<00:00, 21448.98it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 2067765/2067765 [02:08<00:00, 16044.59it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746b8785b71a45409f8778f57e276f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b353af251044fea2891eb472f77a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/17232 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 206777/206777 [00:13<00:00, 15130.78it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 2067765/2067765 [02:37<00:00, 13146.23it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f29e86d37c94948b5459d87e92f5648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a0b34591a348ebaba4ecbdf1331a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/17232 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 206776/206776 [00:14<00:00, 14322.17it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 2067766/2067766 [02:47<00:00, 12380.60it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15330da86604ede9f16a90d5096b7e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128d64bc36644933b7c35b24ec46f629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/17232 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# k-fold validation\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sentence_transformers.cross_encoder.evaluation import CERerankingEvaluator,CEBinaryAccuracyEvaluator,CECorrelationEvaluator\n",
    "\n",
    "data = pd.read_csv('train_0311.csv')\n",
    "\n",
    "kfold = KFold(3, shuffle = True, random_state = 42)\n",
    "num_epochs = 1  \n",
    "model_save_path = \"x-0313\"\n",
    "\n",
    "for train_dataset_n, dev_dataset_n in kfold.split(data):\n",
    "\n",
    "    train_dataset = data.iloc[train_dataset_n]\n",
    "    dev_dataset = data.iloc[dev_dataset_n].sample(frac=0.2)\n",
    "   \n",
    "    #dev_examples=[]\n",
    "    #dev_labels=[]\n",
    "    dev_samples=[]\n",
    "    for idx, row in tqdm(dev_dataset.iterrows(), total=dev_dataset.shape[0], position=0, leave=True):\n",
    "        #dev_examples.append([row['topic'], row['content']])\n",
    "        #dev_labels.append(row['label'])\n",
    "        dev_samples.append(InputExample(texts=[row['topic'], row['content']], label=row['label']))\n",
    "    \n",
    "    train_examples = []\n",
    "    for idx, row in tqdm(train_dataset.iterrows(), total=train_dataset.shape[0], position=0, leave=True):\n",
    "        train_examples.append(InputExample(texts=[row['topic'], row['content']], label=row['label']))\n",
    "        train_examples.append(InputExample(texts=[row['content'],row['topic']], label=row['label']))\n",
    "    train_dataloader = DataLoader(train_examples, shuffle=False, batch_size=240)\n",
    "    \n",
    "    #dev_evaluator = CEBinaryAccuracyEvaluator(sentence_pairs= dev_examples, labels =dev_labels, name='train-eval')\n",
    "    \n",
    "    dev_evaluator = CECorrelationEvaluator.from_input_examples(dev_samples, name='sts-dev')\n",
    "    \n",
    "    x_encoder.fit(train_dataloader=train_dataloader,\n",
    "          epochs=num_epochs,\n",
    "          warmup_steps=warmup_steps,\n",
    "          evaluator=dev_evaluator,\n",
    "          evaluation_steps=30000,\n",
    "          output_path=model_save_path,\n",
    "          show_progress_bar=True\n",
    "         ) \n",
    "\n",
    "    x_encoder.save(\"x-final-0313\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a780f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_encoder.save(\"x-final-0313\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad255038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 fold \n",
    "\n",
    "\n",
    "train_dataset = pd.read_csv('train_0311.csv')\n",
    "\n",
    "\n",
    "# 1 sentence_pairs= dev_examples, labels =dev_labels, - need for DEV dataset\n",
    "dev_examples=[]\n",
    "dev_labels=[]\n",
    "#dev_dataset = train_dataset.sample(frac=0.01).copy()\n",
    "tenperc= len(train_dataset) //100\n",
    "dev_dataset = train_dataset.head(tenperc).copy()\n",
    "for idx, row in tqdm(dev_dataset.iterrows(), total=dev_dataset.shape[0], position=0, leave=True):\n",
    "    dev_examples.append([row['topic'], row['content']])\n",
    "    dev_labels.append(row['label'])\n",
    "print (len(train_dataset)) # now just exclude DEV from train dataset\n",
    "train_dataset = train_dataset[~train_dataset['topic'].isin(dev_dataset['topic'])]\n",
    "print (len(train_dataset))   \n",
    "\n",
    "# 2 InputExample(texts=[row['topic'], row['content']], label=row['label']) - need for TRAIN  dataset\n",
    "train_examples = []\n",
    "for idx, row in tqdm(train_dataset.iterrows(), total=train_dataset.shape[0], position=0, leave=True):\n",
    "    train_examples.append(InputExample(texts=[row['topic'], row['content']], label=row['label']))\n",
    "    #train_examples.append(InputExample(texts=[row['query'], row['pos'], row['neg']]))\n",
    "train_dataloader = DataLoader(train_examples, shuffle=False, batch_size=20)\n",
    "\n",
    "\n",
    "#train_loss = losses.MultipleNegativesRankingLoss(model=x_encoder)\n",
    "\n",
    "# 3 {‘query’: ‘’, ‘positive’: [], ‘negative’: []}  - need this for CERerankingEvaluator\n",
    "#pos_dataset = pd.read_csv('train_0220_pos.csv')\n",
    "#neg_dataset = pd.read_csv('train_0220_neg.csv')\n",
    "#topics_data = pd.read_csv(os.path.join(BASE_DIR,'topics.csv')).query('has_content == True').sample(frac=0.03).copy()\n",
    "#topics_data['merged_text'] = '[CLS]'+ topics_data['title'].fillna('')+'[SEP]'+topics_data['description'].fillna('')\n",
    "#xencoder_eval_list = []\n",
    "#for idx, row in tqdm(topics_data.iterrows(), total=topics_data.shape[0], position=0, leave=True):\n",
    "#    curr_query = row['merged_text'] \n",
    "#    content_pos = pos_dataset[pos_dataset['topic_id_plus'] == row ['id']]['content_pos'].values.tolist()\n",
    "#    content_neg = neg_dataset[neg_dataset['topic_id_plus'] == row ['id']]['content_neg'].values.tolist()\n",
    "#    xencoder_eval_list.append ({'query':curr_query,'positive': content_pos, 'negative': content_neg})\n",
    "\n",
    "#https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/cross-encoder\n",
    "\n",
    "num_epochs = 10  # \n",
    "model_save_path = \"Xencoder-traininig\"\n",
    " \n",
    "from sentence_transformers.cross_encoder.evaluation import CERerankingEvaluator,CEBinaryAccuracyEvaluator\n",
    "dev_evaluator = CEBinaryAccuracyEvaluator(sentence_pairs= dev_examples, labels =dev_labels, name='train-eval')\n",
    "#dev_evaluator = CERerankingEvaluator(xencoder_eval_list, name='train-eval',write_csv=True)  \n",
    "\n",
    "x_encoder.fit(train_dataloader=train_dataloader,\n",
    "          epochs=num_epochs,\n",
    "          warmup_steps=warmup_steps,\n",
    "          evaluator=dev_evaluator,\n",
    "          evaluation_steps=30000,\n",
    "          output_path=model_save_path,\n",
    "          show_progress_bar=True\n",
    "         ) \n",
    "\n",
    "x_encoder.save(\"Xencoder-0313\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005e0e48",
   "metadata": {},
   "source": [
    "# 4 re-generate examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff7ed067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5832be91cca14eea8bd74f89f7cd62b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf3f46968d94109a4bc3da96f045f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 61517/61517 [22:49<00:00, 44.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/input/sentence-transformers')\n",
    "sys.path.append('/kaggle/input/sentence-embedding-models')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from sentence_transformers import SentenceTransformer,InputExample,util, CrossEncoder,losses\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import losses\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "BASE_DIR = '/kaggle/input/learning-equality-curriculum-recommendations'\n",
    "BASE_DIR = 'C:/learning'\n",
    "\n",
    "content_data = pd.read_csv(os.path.join(BASE_DIR,'content.csv'))\n",
    "correlations_data = pd.read_csv(os.path.join(BASE_DIR,'correlations.csv'))\n",
    "topics_data = pd.read_csv(os.path.join(BASE_DIR,'topics.csv'))\n",
    "sub_data = pd.read_csv(os.path.join(BASE_DIR,'sample_submission_0223.csv'))\n",
    "\n",
    "\n",
    "#model_trained = SentenceTransformer('all-MiniLM-L6-v2-biencoder-trained-final-0304')\n",
    "model_trained = SentenceTransformer('bi-final-0313b')\n",
    "model_trained.max_seq_length = 128\n",
    "\n",
    "# We will create some dictionaries and lists to move between topics, contents and targets\n",
    "t2i = {k:i for i,k in enumerate(topics_data['id'].values.tolist())}\n",
    "i2c = {i:k for i,k in enumerate(content_data['id'].values.tolist())}\n",
    "topics_test = correlations_data['topic_id'].values.tolist()\n",
    "targets_test = correlations_data['content_ids'].values.tolist()\n",
    "\n",
    "\n",
    "#topics_data['merged_text'] = topics_data['title'].fillna(' ') + ' [SEP] ' + topics_data['description'].fillna(' ')\n",
    "#content_data['merged_text'] =  content_data['title'].fillna(' ') + ' [SEP] ' + content_data['description'].fillna(' ') \n",
    "\n",
    "\n",
    "parentmap = topics_data.set_index('id')['title'].squeeze()\n",
    "topics_data['parent_title'] = topics_data['parent'].map(parentmap)\n",
    "topics_data['merged_text'] = topics_data['title'].fillna(' ').str.lower() + ' [SEP] ' + topics_data['description'].fillna(' ').str.lower() \\\n",
    "    +  ' [SEP] ' + topics_data['parent_title'].fillna(' ').str.lower()\n",
    "content_data['merged_text'] =  content_data['title'].fillna(' ').str.lower() + ' [SEP] ' + content_data['description'].fillna(' ').str.lower()\\\n",
    "    +  ' [SEP] ' + content_data['text'].fillna(' ').str.lower().str[:100]\n",
    "\n",
    "\n",
    "topic_emb = model_trained.encode(topics_data['merged_text'],  show_progress_bar = True) \n",
    "content_emb = model_trained.encode(content_data['merged_text'],  show_progress_bar = True)  # 2 минуты\n",
    "\n",
    "\n",
    "import faiss\n",
    "k = 50  # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "index = faiss.IndexFlatL2(content_emb.shape[-1])   # build the index\n",
    "index.add(content_emb) # add vectors to the index\n",
    "D, I = index.search(topic_emb, k) # actual search\n",
    "\n",
    "topic_knn = [' '.join([i2c[x] for x in p]) for p in I.tolist()]\n",
    "predictions = [topic_knn[t2i[i]] for i in topics_test]\n",
    "\n",
    "\n",
    "\n",
    "t_list=[]\n",
    "c_list=[]\n",
    "l_list=[]\n",
    "\n",
    "for tidx, j in tqdm(enumerate(predictions), total=len(predictions), position=0, leave=True):\n",
    "    topic_id= correlations_data.iloc[tidx]['topic_id']\n",
    "    query = topics_data.loc[topics_data['id']==topic_id]['merged_text'].fillna(' ').values[0]\n",
    "    \n",
    "    a = predictions[tidx].split()\n",
    "    b = targets_test[tidx].split()\n",
    "    a = [x for x in a if x not in b]    # remove all correct answers from a - keep mistakes only\n",
    "    \n",
    " \n",
    "    mask = content_data['id'].isin(a)\n",
    "    neg_list = content_data.loc[mask]['merged_text'].to_list()\n",
    "    for i in neg_list:\n",
    "        t_list.append(query)\n",
    "        c_list.append(i)\n",
    "        l_list.append(0)        \n",
    "   \n",
    "    \n",
    "    mask = content_data['id'].isin(b)\n",
    "    pos_list = content_data.loc[mask]['merged_text'].to_list()\n",
    "    for i in pos_list:\n",
    "        t_list.append(query)\n",
    "        c_list.append(i)\n",
    "        l_list.append(1)\n",
    "        \n",
    "\n",
    "train_dataset = pd.DataFrame(list(zip(t_list, c_list,l_list)), columns = ['topic', 'content','label'])\n",
    "train_dataset.to_csv('train_0311.csv', index = False)\n",
    "print ('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5620e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
