{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aae162e3",
   "metadata": {},
   "source": [
    "# 1 references \n",
    "\n",
    "\n",
    "-https://huggingface.co/blog/how-to-train-sentence-transformers\n",
    "\n",
    "-https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco\n",
    "\n",
    "-https://colab.research.google.com/github/UKPLab/sentence-transformers/blob/master/examples/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb#scrollTo=2J0Zxgw0artg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7029afd",
   "metadata": {},
   "source": [
    "# 2 make negatives (after train - recreate and retrain again !)\n",
    "### - InputExample(texts=[query_text, pos_text, neg_text])\n",
    "### - InputExample(texts=[row['topic'], row['content']], label=row['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "043a0ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data reading\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ac7611761c43bc8baeb3251e190ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1923 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997cc59f765a4a9fadf16e722b369725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 61517/61517 [19:45<00:00, 51.90it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 61517/61517 [00:05<00:00, 11772.92it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, util, models, evaluation, losses, InputExample\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "#bi_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "bi_encoder = SentenceTransformer('bi-final-0312')\n",
    "\n",
    "bi_encoder.max_seq_length = 128 \n",
    "top_k = 50 # \n",
    "\n",
    "BASE_DIR = '/kaggle/input/learning-equality-curriculum-recommendations'\n",
    "BASE_DIR = 'C:/learning'              # временно, для отладки\n",
    "\n",
    "print ('data reading')\n",
    "content_data = pd.read_csv(os.path.join(BASE_DIR,'content.csv'))\n",
    "correlations_data = pd.read_csv(os.path.join(BASE_DIR,'correlations.csv'))\n",
    "topics_data = pd.read_csv(os.path.join(BASE_DIR,'topics.csv'))\n",
    "sub_data = pd.read_csv(os.path.join(BASE_DIR,'sample_submission.csv'))\n",
    "\n",
    "#RFC0220 - only with content\n",
    "topics_data = topics_data.query('has_content == True').reset_index(drop=True)\n",
    "\n",
    "#RFC0220 - new tokens and context\n",
    "topics_data['level'] = topics_data['level'].astype(str)\n",
    "\n",
    "#topics_data['merged_text'] = topics_data['title'].fillna(' ')\n",
    "#content_data['merged_text'] =  content_data['title'].fillna(' ')\n",
    "\n",
    "#topics_data['merged_text'] = topics_data['title'].fillna(' ') + ' [SEP] ' + topics_data['description'].fillna(' ')\n",
    "#content_data['merged_text'] =  content_data['title'].fillna(' ') + ' [SEP] ' + content_data['description'].fillna(' ')\n",
    "\n",
    "\n",
    "parentmap = topics_data.set_index('id')['title'].squeeze()\n",
    "topics_data['parent_title'] = topics_data['parent'].map(parentmap)\n",
    "topics_data['merged_text'] = topics_data['title'].fillna(' ').str.lower() + ' [SEP] ' + topics_data['description'].fillna(' ').str.lower() \\\n",
    "    +  ' [SEP] ' + topics_data['parent_title'].fillna(' ').str.lower()\n",
    "content_data['merged_text'] =  content_data['title'].fillna(' ').str.lower() + ' [SEP] ' + content_data['description'].fillna(' ').str.lower()\\\n",
    "    +  ' [SEP] ' + content_data['text'].fillna(' ').str.lower().str[:100]\n",
    "\n",
    "\n",
    "topic_embeddings = bi_encoder.encode(topics_data['merged_text'], convert_to_tensor=True, show_progress_bar = True)  # 2 минуты\n",
    "content_embeddings = bi_encoder.encode(content_data['merged_text'], convert_to_tensor=True, show_progress_bar = True)    # 1 минуты\n",
    "\n",
    "\n",
    "def search_cos_sim(query):\n",
    "    query_embedding = bi_encoder.encode(query, convert_to_tensor=True, show_progress_bar = False)\n",
    "    cos_scores = util.cos_sim(query_embedding, content_embeddings)[0]\n",
    "    hits = torch.topk(cos_scores, k=top_k)\n",
    "    return (hits[1].cpu().tolist())\n",
    "\n",
    "\n",
    "def search_cos_sim2(query):\n",
    "    query_embedding = bi_encoder.encode(query, convert_to_tensor=True, show_progress_bar = False)\n",
    "    cos_scores = util.cos_sim(query_embedding, content_embeddings)[0]\n",
    "    hits_t = torch.topk(cos_scores, k=top_k)\n",
    "    hits = hits_t[1].cpu().tolist()\n",
    "    scores = hits_t[0].cpu().tolist()\n",
    "    # давай понизим шумность датасета - иначе он противоречит сам себе - уберем слишком близкие совпадения из обучения\n",
    "    real_hits=[]\n",
    "    real_scores=[]\n",
    "    for idx, element in enumerate(hits):\n",
    "        if scores[idx]<=0.99:\n",
    "            real_hits.append(element)\n",
    "            real_scores.append(scores[idx])\n",
    "    return (real_hits, real_scores)\n",
    "\n",
    "# negative examples\n",
    "t_list=[]\n",
    "c_list=[]\n",
    "idx_list =[]\n",
    "sc_list=[]\n",
    "for t_idx, row in tqdm(topics_data.iterrows(), total=topics_data.shape[0], position=0, leave=True):\n",
    "    hits,scores = search_cos_sim2 (row['merged_text'])\n",
    "    counter_neg = 0\n",
    "    for hits_idx, hit in enumerate(hits):\n",
    "        idx_list.append(row['id'])   # здесь счетчик НЕ добавляем в список -- ДОБАВИМ ПОЗЖЕ\n",
    "        t_list.append(row['merged_text'])\n",
    "        c_list.append(content_data.iloc[hit]['merged_text'])  #  hit or hit['corpus_id'] is the index in content_data \n",
    "        sc_list.append(scores[hits_idx])\n",
    "        counter_neg +=1\n",
    "        \n",
    "# positive examples\n",
    "ti_list=[]\n",
    "ci_list=[]\n",
    "idxi_list =[]\n",
    "sci_list =[]\n",
    "for idx, row in tqdm(correlations_data.iterrows(), total=correlations_data.shape[0], position=0, leave=True):\n",
    "    counter_pos = 0\n",
    "    for word in row['content_ids'].split():\n",
    "        idxi_list.append(row['topic_id']+\"_\"+str(counter_pos))  # здесь счетчик СРАЗУ добавляем в список\n",
    "        ti_list.append(row['topic_id'])\n",
    "        ci_list.append(word)\n",
    "        sci_list.append(1)\n",
    "        counter_pos +=1\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "902b021c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 61517/61517 [48:26<00:00, 21.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3051747 3051747 should be equal at the end of procedure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# простой cross-join не нужен - нужна стыковка следующего типа \n",
    "#df1 = DataFrame({'key':[1,2,3,1,2,3,1,2], 'col1':[1,2,3,4,5,6,7,8]})\n",
    "#df2 = DataFrame({'key':[1,2,3], 'col2':[10,11,12]})\n",
    "#merge(df1, df2,on='key')\n",
    "\n",
    "idx_list2=[]\n",
    "temp_topic_id =''\n",
    " \n",
    "for t_idx, element in tqdm(topics_data.iterrows(), total=topics_data.shape[0], position=0, leave=True):\n",
    "    # сколько получилось позитивных примеров для данного топика \n",
    "    temp_count_pos = ti_list.count(element['id'])\n",
    "    # сколько получилось негативных примеров для данного топика\n",
    "    temp_count_neg = idx_list.count(element['id'])\n",
    "    temp_counter = 0\n",
    "    for i in range (temp_count_neg):\n",
    "        sub_element = element['id'] + \"_\" + str(temp_counter)\n",
    "        idx_list2.append(sub_element)\n",
    "        temp_counter +=1\n",
    "        if temp_counter >= temp_count_pos:temp_counter =0\n",
    "   \n",
    "print(len(idx_list2), len(idx_list), 'should be equal at the end of procedure')\n",
    "idx_list = idx_list2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7277255a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data converting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dmitriy\\AppData\\Local\\Temp\\ipykernel_7776\\3189652152.py:32: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  pos_dataset = pos_dataset.append(neg_dataset)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done - 1203\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print ('data converting')\n",
    "        \n",
    "neg_dataset = pd.DataFrame(list(zip(t_list, c_list,idx_list,sc_list)), columns = ['topic', 'content_neg','topic_id_plus','neg_score'])  \n",
    "\n",
    "pos_dataset = pd.DataFrame(list(zip(ti_list, ci_list, idxi_list,sci_list)), columns = ['topic_id', 'content_id','topic_id_plus','pos_score'])   \n",
    "pos_dataset = pd.merge(pos_dataset,topics_data, left_on='topic_id', right_on='id', how='left').loc[:,['merged_text','content_id','topic_id_plus','pos_score']].copy()\n",
    "pos_dataset = pos_dataset.rename({'merged_text': 'topic'}, axis=1)\n",
    "pos_dataset = pd.merge(pos_dataset,content_data, left_on='content_id', right_on='id', how='left').loc[:,['topic','merged_text','topic_id_plus','pos_score']].copy()\n",
    "pos_dataset = pos_dataset.rename({'merged_text': 'content_pos'}, axis=1)\n",
    "\n",
    "# InputExample(texts=[query_text, pos_text, neg_text])   === 0220 - основной для биэнкодера \n",
    "#train_dataset = pd.merge(pos_dataset, neg_dataset, on='topic_id_plus')\n",
    "# простой cross-join не нужен - нужна стыковка особого типа - см предыдущую ячейку \n",
    "train_dataset=pd.merge(neg_dataset,pos_dataset,left_on='topic_id_plus', right_on='topic_id_plus', how='left')\n",
    "\n",
    "train_dataset = train_dataset.rename({'topic_x': 'query'}, axis=1)\n",
    "train_dataset = train_dataset.rename({'content_pos': 'pos'}, axis=1)\n",
    "train_dataset = train_dataset.rename({'content_neg': 'neg'}, axis=1)\n",
    "train_dataset.drop(['topic_y','topic_id_plus','pos_score','neg_score'], axis=1, inplace=True)\n",
    "train_dataset.to_csv('train_0220.csv', index = False)     #query,pos,neg\n",
    "pos_dataset.to_csv('train_0220_pos.csv', index = False)\n",
    "neg_dataset.to_csv('train_0220_neg.csv', index = False)\n",
    "\n",
    "\n",
    "# InputExample(texts=[row['topic'], row['content']], label=row['label'])  === 0208\n",
    "pos_dataset = pos_dataset.rename({'content_pos': 'content'}, axis=1)\n",
    "pos_dataset = pos_dataset.rename({'pos_score': 'label'}, axis=1)\n",
    "pos_dataset.drop(['topic_id_plus'], axis=1, inplace=True)\n",
    "neg_dataset = neg_dataset.rename({'content_neg': 'content'}, axis=1)\n",
    "neg_dataset = neg_dataset.rename({'neg_score': 'label'}, axis=1)\n",
    "neg_dataset.drop(['topic_id_plus'], axis=1, inplace=True)\n",
    "pos_dataset = pos_dataset.append(neg_dataset)\n",
    "pos_dataset.to_csv('train_0208.csv', index = False)     # for x-encoder - not used now\n",
    "print ('done - 1203')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f39ce4",
   "metadata": {},
   "source": [
    "# 3 train biencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48aaa2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name C:\\Users\\Dmitriy/.cache\\torch\\sentence_transformers\\microsoft_deberta-v3-small. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at C:\\Users\\Dmitriy/.cache\\torch\\sentence_transformers\\microsoft_deberta-v3-small were not used when initializing DebertaV2Model: ['mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "C:\\Users\\Dmitriy\\.conda\\envs\\cuda\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, util, models, evaluation, losses, InputExample\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sentence_transformers.datasets import NoDuplicatesDataLoader\n",
    "\n",
    "\n",
    "\n",
    "#bi_encoder = SentenceTransformer('bi-final-0313')\n",
    "bi_encoder = SentenceTransformer('microsoft/deberta-v3-small')\n",
    "bi_encoder.max_seq_length = 128\n",
    "top_k = 50\n",
    "\n",
    "\n",
    "data= pd.read_csv('train_0220.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ff29796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/UKPLab/sentence-transformers/issues/336\n",
    "import logging\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from typing import List, Union\n",
    "import math\n",
    "from tqdm.autonotebook import trange\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sentence_transformers.evaluation import SentenceEvaluator\n",
    "from sentence_transformers import InputExample\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class LossEvaluator(SentenceEvaluator):\n",
    "\n",
    "    def __init__(self, loader, loss_model: nn.Module = None, name: str = '', log_dir: str = None,\n",
    "                 show_progress_bar: bool = True, write_csv: bool = True):\n",
    "\n",
    "        # move model to gpu\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        loss_model.to(self.device)\n",
    "\n",
    "        self.loader = loader\n",
    "        self.write_csv = write_csv\n",
    "        self.logs_writer = SummaryWriter(log_dir=log_dir)\n",
    "        self.name = name\n",
    "        self.loss_model = loss_model\n",
    "        if show_progress_bar is None:\n",
    "            show_progress_bar = (\n",
    "                    logger.getEffectiveLevel() == logging.INFO or logger.getEffectiveLevel() == logging.DEBUG)\n",
    "        self.show_progress_bar = show_progress_bar\n",
    "\n",
    "        self.csv_file = \"loss_evaluation\" + (\"_\" + name if name else '') + \"_results.csv\"\n",
    "        self.csv_headers = [\"epoch\", \"steps\", \"loss\"]\n",
    "\n",
    "    def __call__(self, model, output_path: str = None, epoch: int = -1, steps: int = -1) -> float:\n",
    "\n",
    "        self.loss_model.eval()\n",
    "\n",
    "        loss_value = 0\n",
    "        self.loader.collate_fn = model.smart_batching_collate\n",
    "        num_batches = len(self.loader)\n",
    "        data_iterator = iter(self.loader)\n",
    "\n",
    "               \n",
    "        with torch.no_grad():\n",
    "            for _ in trange(num_batches, desc=\"Iteration\", smoothing=0.05, disable=not self.show_progress_bar):\n",
    "                sentence_features, labels = next(data_iterator)\n",
    "                # move data to gpu\n",
    "                for i in range(0, len(sentence_features)):\n",
    "                    for key, value in sentence_features[i].items():\n",
    "                        sentence_features[i][key] = sentence_features[i][key].to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                loss_value += self.loss_model(sentence_features, labels).item()\n",
    "\n",
    "        final_loss = loss_value / num_batches\n",
    "        print(loss_value )\n",
    "        if output_path is not None and self.write_csv:\n",
    "\n",
    "            csv_path = os.path.join(output_path, self.csv_file)\n",
    "            output_file_exists = os.path.isfile(csv_path)\n",
    "\n",
    "            with open(csv_path, newline='', mode=\"a\" if output_file_exists else 'w', encoding=\"utf-8\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                if not output_file_exists:\n",
    "                    writer.writerow(self.csv_headers)\n",
    "\n",
    "                writer.writerow([epoch, steps, final_loss])\n",
    "\n",
    "            # ...log the running loss\n",
    "            self.logs_writer.add_scalar('val_loss',\n",
    "                                        final_loss,\n",
    "                                        steps)\n",
    "\n",
    "        self.loss_model.zero_grad()\n",
    "        self.loss_model.train()\n",
    "\n",
    "        return final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a67c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 2034498/2034498 [01:49<00:00, 18659.74it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 101725/101725 [00:05<00:00, 19052.05it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fafa600f5b6c41dba0796b6749028ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f30ee54e5504fa39d85c0b62cc06b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/254313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348d7a755fbd4d54a2ac9332c894c888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/12716 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12614.933639228344\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e896ef7ec44173be641798ca150a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/12716 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9695.52869521454\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ea8ddfb5594b7db9492b67777b0042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/12716 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8445.197329588234\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee340a99b1e143a6a91603452c403d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/12716 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7622.469901826233\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be3cad0508624e1aa665d16236c69f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/12716 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7179.019978321157\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87acff852d18412c9d26b3342891bc1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/12716 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6775.680907540023\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aae03e14e9f4945a521c6abe145d940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/12716 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6476.465206870809\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9539f76fab492a9db7629b369f208a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/12716 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6180.376741409767\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e180a67b4f340b9931a630c4ec9bd84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/12716 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5946.843653499731\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a25174e92244685af0857acac7f8bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/12716 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5786.933847125212\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc60a388c8fd4bcc8cf5ea26ba4fd232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/12716 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5420.493810238782\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a759a2e3b84d8b9133f0850d80c6bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/12716 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5375.635744090658\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b531faca0244768239220ed710d9ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/12716 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5073.799450573744\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb202b9dab3c420e9fffd9b12c1c7187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/12716 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4981.387567618163\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208643225cc946cfa4db0e07b918794f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/12716 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4915.075846517808\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c09fcee8ac485aaf90e357a3142d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/12716 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4651.1907454248285\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a3eee501bf4046bc670ed63fc119c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/12716 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4565.260376084247\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400a7992a0ad4cbaab672e436b93e16b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/12716 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4548.202676246408\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9422774b52e044548777c518ba02f5fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/12716 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4397.7673950672615\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba9b953828b647bb8c1937baac752343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/12716 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4294.817836695904\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53d9890e4364382acf43e4eba7df8ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/12716 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4273.834538573166\n"
     ]
    }
   ],
   "source": [
    "# k-fold validation\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(3, shuffle = True, random_state = 42)\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "lr = 2e-5\n",
    "model_save_path = \"bi-0314\"\n",
    "#train_loss = losses.ContrastiveLoss(model=bi_encoder)\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model=bi_encoder)\n",
    "\n",
    "\n",
    "for train_dataset_n, dev_dataset_n in kfold.split(data):\n",
    "\n",
    "    train_dataset = data.iloc[train_dataset_n]\n",
    "    dev_dataset = data.iloc[dev_dataset_n].sample(frac=0.1)\n",
    "\n",
    "    train_examples = []\n",
    "    for idx, row in tqdm(train_dataset.iterrows(), total=train_dataset.shape[0], position=0, leave=True):\n",
    "        #train_examples.append(InputExample(texts=[row['topic'], row['content']], label=row['label']))\n",
    "        train_examples.append(InputExample(texts=[row['query'], row['pos'], row['neg']]))\n",
    "    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=8)  # 120\n",
    "\n",
    "    dev_examples = []\n",
    "    for idx, row in tqdm(dev_dataset.iterrows(), total=dev_dataset.shape[0], position=0, leave=True):\n",
    "        #dev_examples.append(InputExample(texts=[row['topic'], row['content']], label=row['label']))\n",
    "        dev_examples.append(InputExample(texts=[row['query'], row['pos'], row['neg']]))\n",
    "    dev_loader = DataLoader(dev_examples,shuffle=True, batch_size=8)  #110\n",
    "    dev_evaluator = LossEvaluator(dev_loader, loss_model=train_loss, show_progress_bar= True,log_dir='logs/', name='dev')\n",
    "\n",
    "    warmup_steps = int(len(train_dataloader) * num_epochs * 0.1) #10% of train data\n",
    "\n",
    "    bi_encoder.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "              epochs=num_epochs,\n",
    "              evaluator=dev_evaluator,\n",
    "              evaluation_steps=5000,     \n",
    "              warmup_steps=warmup_steps,\n",
    "              use_amp=True,\n",
    "              optimizer_params = {'lr': lr},\n",
    "              checkpoint_path=model_save_path,\n",
    "              checkpoint_save_steps=len(train_dataloader),\n",
    "              show_progress_bar=True\n",
    "              ) \n",
    "\n",
    "bi_encoder.save(\"bi-final-0314\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3b47ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 fold training \n",
    "\n",
    "#bi_encoder = SentenceTransformer('bi-final-0308')\n",
    "#bi_encoder.max_seq_length = 128 \n",
    "\n",
    "train_dataset = data\n",
    "dev_dataset = data.sample(frac=0.05)\n",
    "\n",
    "\n",
    "train_examples = []\n",
    "for idx, row in tqdm(train_dataset.iterrows(), total=train_dataset.shape[0], position=0, leave=True):\n",
    "    #train_examples.append(InputExample(texts=[row['topic'], row['content']], label=row['label']))\n",
    "    train_examples.append(InputExample(texts=[row['query'], row['pos'], row['neg']]))\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=120)  # 120\n",
    "\n",
    "dev_examples = []\n",
    "for idx, row in tqdm(dev_dataset.iterrows(), total=dev_dataset.shape[0], position=0, leave=True):\n",
    "    #dev_examples.append(InputExample(texts=[row['topic'], row['content']], label=row['label']))\n",
    "    dev_examples.append(InputExample(texts=[row['query'], row['pos'], row['neg']]))\n",
    "dev_loader = DataLoader(dev_examples,shuffle=True, batch_size=110)  #110\n",
    "dev_evaluator = LossEvaluator(dev_loader, loss_model=train_loss, show_progress_bar= True,log_dir='logs/', name='dev')\n",
    "\n",
    "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1) #10% of train data\n",
    "\n",
    "bi_encoder.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          epochs=num_epochs,\n",
    "          evaluator=dev_evaluator,\n",
    "          evaluation_steps=5000,     \n",
    "          warmup_steps=warmup_steps,\n",
    "          use_amp=True,\n",
    "          optimizer_params = {'lr': lr},\n",
    "          checkpoint_path=model_save_path,\n",
    "          checkpoint_save_steps=len(train_dataloader),\n",
    "          show_progress_bar=True\n",
    "          ) \n",
    "\n",
    "bi_encoder.save(\"bi-final-0313b\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab5b62e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_encoder.save(\"bi-final-0313b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c90b59",
   "metadata": {},
   "source": [
    "# backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ef3b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print ('data converting')\n",
    "        \n",
    "neg_dataset = pd.DataFrame(list(zip(t_list, c_list,idx_list,sc_list)), columns = ['topic', 'content_neg','topic_id_plus','neg_score'])  \n",
    "\n",
    "pos_dataset = pd.DataFrame(list(zip(ti_list, ci_list, idxi_list,sci_list)), columns = ['topic_id', 'content_id','topic_id_plus','pos_score'])   \n",
    "pos_dataset = pd.merge(pos_dataset,topics_data, left_on='topic_id', right_on='id', how='left').loc[:,['merged_text','content_id','topic_id_plus','pos_score']].copy()\n",
    "pos_dataset = pos_dataset.rename({'merged_text': 'topic'}, axis=1)\n",
    "pos_dataset = pd.merge(pos_dataset,content_data, left_on='content_id', right_on='id', how='left').loc[:,['topic','merged_text','topic_id_plus','pos_score']].copy()\n",
    "pos_dataset = pos_dataset.rename({'merged_text': 'content_pos'}, axis=1)\n",
    "\n",
    "# InputExample(texts=[query_text, pos_text, neg_text])   === 0220 - основной для биэнкодера \n",
    "#RFC 0220 - cross join\n",
    "#train_dataset = pd.merge(pos_dataset, neg_dataset, on='topic_id_plus')\n",
    "#train_dataset=pd.merge(neg_dataset,pos_dataset,left_on='topic_id_plus', right_on='topic_id_plus', how='left')\n",
    "# кросс пересечение на 50 - получилось записей 13823680\n",
    "# НЕ кросс пересечение на 100 - получилось записей 6115865\n",
    "\n",
    "train_que=[]\n",
    "train_pos=[]\n",
    "train_neg=[]\n",
    "temp_counter = 0 \n",
    "\n",
    "for idx, row in tqdm(neg_dataset.iterrows(), total=neg_dataset.shape[0], position=0, leave=True):\n",
    "    temp_topic = row['topic_id_plus']\n",
    "    train_que.append(row['topic'])\n",
    "    train_neg.append(row['content_neg'])\n",
    "    temp_pos_dataset = pos_dataset[pos_dataset['topic_id_plus'] == temp_topic].copy().reset_index()\n",
    "    temp_len_pos= len(temp_pos_dataset)\n",
    "    if temp_counter >= temp_len_pos: temp_counter = 0\n",
    "    train_pos.append(temp_pos_dataset.iloc[temp_counter]['content_pos'])\n",
    "    temp_counter +=1\n",
    "train_dataset = pd.DataFrame(list(zip(train_que, train_pos,train_neg)), columns = ['query', 'neg','pos'])  \n",
    "\n",
    "# temp block!!!\n",
    "\n",
    "train_dataset.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
